\documentclass[hoptionsi,review,format=sigplan]{acmart}
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ...
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex
\usepackage{minted}
\setminted[haskell]{escapeinside=@@}

\newenvironment{code}
 {\VerbatimEnvironment
  \begin{minted}[fontsize=\small]{haskell}}
 {\end{minted}}

\newenvironment{codex}
 {\VerbatimEnvironment
  \begin{minted}[fontsize=\small]{haskell}}
 {\end{minted}}
 
\newcommand{\hs}{\mintinline[fontsize=\footnotesize]{haskell}}

\newtheorem{theorem}{Theorem}

\title[Traversals of Infinite Structures]{A Predictable Outcome: An Investigation of Traversals of Infinite Structures}
\author{Gershom Bazerman}
\affiliation{%
  \institution{Awake Security}
  \city{San Jose}
  \country{USA}
}

\begin{comment}
\begin{code}
{-# LANGUAGE DeriveFunctor, StandaloneDeriving, UndecidableInstances, PolyKinds, MultiParamTypeClasses, FlexibleInstances, TypeFamilies, RankNTypes, TupleSections #-}

module Predictable where

import Data.Functor.Compose
import Control.Monad.Reader
import Control.Monad.Writer
import Control.Monad.State

\end{code}
\end{comment}

%\date{}							% Activate to display a given date or no date

\begin{abstract}
Functors with an instance of the \hs{Traversable} type class can be thought of as data structures which permit a traversal of their elements. This has been made precise by the correspondence between traversable functors and finitary containers (aka polynomial functors). This correspondence was established in the context of total, necessarily terminating, functions. However, the Haskell language is non-strict and permits functions that do not terminate. It has long been observed that traversals can at times, in practice, operate over infinite lists, for example in distributing the Reader applicative. The result of such a traversal remains an infinite structure, however it nonetheless is productive -- i.e. successive amounts of finite computation yield either termination or successive results. To investigate this phenomenon, we draw on tools from guarded recursion, making use of equational reasoning internal to Haskell. This is done by introducing a formal \hs{Later} modality in the form of an applicative functor, with an associated \hs{Eventually} monad, and introducing a bisimilarity relation on the latter. Using these tools, it transpires that for a traversal to be possibly infinite is less a restriction on the structure being traversed, but rather a restriction on the applicative performing the traversal -- in particular such an applicative must commute with \hs{Later} up to bisimilarity. We term such functors \hs{Predictable} and investigate their structure, providing a range of examples, and showing how this form of analysis allows fine-grained distinctions to be made on the behavior of various operations.
\end{abstract}

\begin{document}
\maketitle

\section{Introduction}
Traversable functors, first introduced by [CITE MCBRIDE], provide a sort of "complement" to the Applicative type class. While the Applicative type class picks out functors which have a (closed) lax-monoidal structure, the Traversable type class picks out functors which can distribute over this monoidal structure. We recall below the definitions of Applicative and Traversable which we will be generalizing in this paper:

\begin{codex}
class Functor f => Applicative f where
     pure :: a -> f a
    (<*>) :: f (a -> b) -> f a -> f b      
    
class Functor t => Traversable t where
    sequence :: Applicative f => t (f a) -> f (t a)
\end{codex}


Typically the Traversable type class is given by an operation \hs{traverse} rather than \hs{sequence} but the former is given by \hs{traverse = sequence . fmap} and the latter is conceptually clearer for our purposes.

By results (independently) of [GIBBONS, OCONNOR] it was established that in a total setting, traversable functors correspond to finitary containers, which is to say that any traversable functor is equivalent to one given as the sum of finite products of its underlying element. Recursive structures in the Haskell language are however, not necessarily finitary. For example, the list functor includes lists which have infinite numbers of elements, aka streams. Often, as defined in Haskell, traversals naturally operate on such infinite structures. As a trivial example, \hs{sequence (repeat id) 1}, which makes use of the \hs{Reader} applicative yields an infinite list \hs{[1,1,1...]}.

However, not all applicative functors are conducive to infinite traversals. For example, \hs{sequence (repeat (Just 1))} yields $\bot$.

And more interestingly yet, some applicative functors allow traversals that are only productive (non-bottom) in some \emph{portion} of their results. For example, making use of the \hs{State} monad:

\begin{codex}
runState (sequence (repeat (modify (+1) >> get))) 0
\end{codex}

yields a tuple whose first projection is the infinite list \hs{[1,2,3...]} and whose second projection is $\bot$.

Internal algebraic reasoning about functions like these can be rather tricky. This paper will draw on the toolkit of guarded recursion to provide a straightforward way of thinking about such functions using standard equational reasoning, and then, making use of this tool, describe a class of functors, which we term \hs{Predictable} functors, that are suited to performing infinite traversals.

\section{Equipping Haskell with a Formal Later modality}

In 2000, Nakhano introduced a typed modal logic for self referential formulae which took semantics in a typed lambda calculus with a modality for guarded recursion. In such a setting, types are able to capture when certain data must be evaluated in order for further evaluation to proceed. A term of type \hs{a} which cannot yet be evaluated is given the type \hs{Later a}, expressing that it may be passed around abstractly, but its structure is not yet necessarily available for computation. In such a setting, there is necessarily no general function \hs{forall a. Later a -> a}, nor is there a general fixpoint operator. However, guarded recursion is allowed in the form of a function \hs{lfix :: (Later a -> a) -> a}. This says "if you give provide a function that "embeds" a \hs{Later a} into an \hs{a}, then it can be iterated." Such a function allows a fixpoint because the input function, by construction, cannot evaluate the thing it is given, only manipulate it abstractly. Hence, the \hs{Later} modality makes explicit the rule of thumb that programmers who work with the fixpoint operator have already internalized.

In a system like this, necessarily finite and possibly infinite data are distinguished through the use of the \hs{Later} modality. Consider for example, the types
\begin{code}
data List a = 
     LNil 
   | LCons a (List a)
   
data Stream a = 
     Nil
   | Cons a (Later (Stream a))
\end{code}
The former is necessarily finite, as without a general fixpoint operator, no infinite inhabitant can be constructed. However, the latter is possibly infinite. For example, \hs{lfix (Cons 1)} will produce an infinite stream of ones.

 By an insight of Atkey and McBride, aside from \hs{lfix}, the later modality has exactly the operations of an applicative functor. In particular, we have:
 
 \begin{codex}
 later :: a -> Later a
 lap :: Later (a -> b) -> Later a -> Later b
 \end{codex}
 
Further, in combination these induce functoriality. These operations come with natural intuitions corresponding to the intended meaning of \hs{Later}. "If I know something know, I still know it later. If I will have a function later, and I will have a value later, then later I also will be able to have the result of applying that function to that value."
 
The recognition of \hs{Later} as an applicative functor means that one can program in a fragment of Haskell which enjoys the same properties as a type system with genuine guarded recursion -- i.e. infinite structures can be manipulated in a fashion such that no function yields $\bot$. The recipe for doing so is painfully simple. One simply introduces an abstract datatype equipped with the proper type class instances, and an \hs{lfix} operator, and then does not export its eliminator. 

This is to say one introduces a module supplying the following, and for all code in the fragment we are reasoning about, does not import the constructor \hs{Later}, thus enforcing abstraction.

\begin{code}
newtype Later a = Later a deriving Functor

instance Applicative Later where
  pure = Later
  Later f <*> Later x = Later (f x)

lfix :: (Later a -> a) -> a
lfix f = fix (f . pure)
\end{code}

Further, for all code in sight, one adheres to the discipline of not allowing any direct recursion to occur, and instead using the \hs{lfix} operator uniformly. Working with this sort of discipline without compiler support is not necessarily a good approach for genuine programming in the large. However, for small-scale equational reasoning it suffices, as the condition is straightforward to check by hand.

In the course of this paper, we will refer to terms written with the above discipline (making use of \hs{Later} and \hs{lfix} with no explicit recursion) as guarded recursive terms, and generally to code written with this discipline as in the guarded recursive fragment. Additionally, we will refer to datatypes which make use of the \hs{Later} modality (such as \hs{Stream}) as guarded datatypes, types of code written in the guarded recursive fragment as guarded recursive types, and functions written in this fragment (whether or not they are recursive) as guarded recursive functions.

As an example of working in guarded recursive fragment, we present a function for interleaving possibly infinite streams, as well as merging them with a (truncated) zip. We leave it as an exercise to verify that, e.g., the \hs{reverse} function cannot be written.

\begin{code}
sinterleave :: Stream a -> Stream a -> Stream a
sinterleave = lfix $ \f s1 s2 -> case s1 of
  (Cons x xs) -> Cons x (f <*> pure s2 <*> xs)
  _ -> s2

szip :: Stream a -> Stream b -> Stream (a, b)
szip = lfix $ \f s1 s2 -> case (s1, s2) of
  (Cons x xs, Cons y ys) -> Cons (x,y) (f <*> xs <*> ys)
  _ -> Nil
\end{code}

We will also want to consider functions which are only possibly productive -- i.e. where there may be arbitrary sequences of \hs{Later} applications. These can be captured by a \hs{Delay} type, following Capretta.

\begin{code}
data Delay a = Now a | Wait (Later (Delay a))
\end{code}

Using this type, one can, for example write a function to compute the last element (should it exist) of a possibly infinite stream.

\begin{code}
slast :: Stream a -> Delay (Maybe a)
slast = go Nothing
  where go = lfix $ \f def s1 ->
         case s1 of
          (Cons x xs) -> Wait $ f <*> pure (Just x) <*> xs
          Nil -> Now def
\end{code}

This type can also be used to capture structures that are possibly productive, such as possibly productive infinite streams:

\begin{code}
data PStream a =
      PNil
    | PWait (Later (PStream a))
    | PCons a (PStream a)
\end{code}


\section{Predictable functors and infinite traversals}

With the above tools in hand, we are ready to provide an informal introduction to the central idea of this paper. The forward implication of the correspondence between traversable functors and finitary containers is simply the observation that traversable functors are closed under arbitrary sums and finite products [CITE JASK]. The goal is to extend traversals to handle guarded types such as \hs{Stream}. Hence, we need to close "infinite-traversable" functors under sum, product, and also composition with \hs{Later}. This amounts to having (an appropriately lawful) function \hs{Later (t (f a)) -> f (Later (t a))}. But this decomposes into the composition of \\ \hs{fmap sequence :: Later (t (f a)) -> Later (f (t a))} with a function \hs{predict :: Later (f a) -> f (Later a)}. This latter function makes no mention of \hs{t} and so it is effectively not a property of traversable functors, but rather of the applicative functors used to traverse them. This motivates defining a type class, \hs{Predictable} as follows:

% TODO jask has arb sum, finite product. but we have arb sub, sort of arb product? need better grasp on guarded datatypes

\begin{code}
class Predictable f where
  predict :: Later (f a) -> f (Later a)
\end{code}

And, in turn, we can now define a candidate type class for infinite-traversable functors:

\begin{code}
class ITraversable t where
  isequence :: (Applicative f, Predictable f) => t (f a) -> f (t a)
\end{code}

Since these classes are for reasoning in the guarded recursive fragment, we require that their instances be given only using tools from that fragment.

As an example, here is the \hs{ITraversable} instance for \hs{Stream}.

\begin{code}
instance ITraversable Stream where
  isequence = lfix $ \rec x -> case x of
     Nil -> pure Nil
     Cons a s -> 
         Cons
           <$> a 
           <*> predict (rec <*> s)
\end{code}

Syntactically, it looks nearly the same as the \hs{Traversable} instance for lists, but it makes a judicious use of \hs{predict} to align the \hs{Later} uses in the course of the traversal.

In general, for any strictly positive recursive datatype, one can construct a related, "potentially infinite" guarded datatype by syntactically guarding each recursive occurrence by \hs{Later}. Since any \hs{Traversable} datatype can be written as such a strictly positive type, then it follows that for every \hs{Traversable} datatype there is a related guarded datatype which can be given an instance of \hs{ITraversable}, which we will refer to as the recursively guarded variant of the original type.

As a further example of this, we give the a binary tree with labeled leafs, as well as the recursively guarded version of it, and a corresponding \hs{ITraversable} instance for the latter:

\begin{code}
-- "normal" tree
data Tree a = Leaf a | Branch (Tree a) (Tree a)

-- guarded tree
data ITree a = 
     ILeaf a 
   | ITBranch (Later (ITree a)) (Later (ITree a))

-- corresponding infinite traversable instance
instance ITraversable ITree where
  isequence = lfix $ \rec x -> case x of
     ILeaf a -> ILeaf <$> a
     ITBranch x y -> 
             ITBranch 
          <$> predict (rec <*> x)
          <*> predict (rec <*> y)
\end{code}

The next question to consider is when an applicative functor may be given a valid \hs{Predictable} instance. However, this requires specifying what a "valid" predictable instance is. In turn, to do so, some new tools must be developed. The general idea is that our use of types to track guarded recursion in a fragment of Haskell is merely a way of "annotating" existing code to make it make sense. As such, the function \hs{predict} is intended to be merely an accounting device, witnessing some property of the underlying functor, and not to do actual computation. So we might wish to require it be an isomorphism on types. However, in obviously desirable cases it cannot have an inverse. For example, it is easy to send \hs{Later (a -> b)} to \hs{a -> Later b}, but there is no general function going in the other direction. Therefore, we need a notion of equivalence between terms of heterogenous type that suffices to capture the intended semantics -- for this, we introduce something we term bisimilarity by evaluation.

\section{Bisimilarity by evaluation}
% TODO can dive right in to Later a -> a
As a general motivation, we note that \hs{Later} is not a monad, and in particular, there is no function of the form:
\begin{codex}
forall a. Later (Later a) -> Later a
\end{codex} 

Intuitively, if such a thing existed, it would "collapse" all future timesteps into a single timestep. As such, it would allow unguarded recursion as long as it occurred under at least a single \hs{Later}. 

Nonetheless, we wish to use the guarded recursion modality to reason about terms in Haskell -- a nonstrict language with general recursion. As such, we want to consider equivalence between terms of type \hs{Later a} and \hs{Later (Later a)}, for example. The appropriate notion of equivalence between terms in a nonstrict setting, like that between concurrent terms, should be some form of bisimilarity. In essence, we wish terms to be considered equivalent if under some sequence of abstract observations, each contains the same data, with the same causal ordering dependency. 

Put another way, the reason we do not want a morphism from \hs{Later (Later a)} to \hs{Later a} is because this would allow a use of \hs{lfix} to perform unbounded recursion. However, for purposes of reasoning about equivalence, rather than calculation, such a map is reasonable. In fact, we can go further. For purposes of reasoning up to equivalence, a map \hs{Later a -> a} is reasonable as well. In fact, in general, since \hs{Later} is a newtype, there will be an associated map from any type with \hs{Later} involved to one without \hs{Later} involved, which, considered purely as a Haskell type, will be an isomorphism. The general idea is that terms may defined in a total setting where there is no general map \hs{Later a -> a} and all recursion is via \hs{lfix}. However, bisimilarity between terms is calculated by "evaluating" these terms to a nonstrict setting with general recursion, and then calculating equivalence using the standard tools of denotational semantics. Put another way: rather than constructing a theory of bisimilarity on guarded recursive types intended to capture when they are equivalent in a nonstrict, recursive setting, we simply construct a procedure for evaluating such types to a nonstrict, recursive setting, and directly reason about their equivalence using tools that already exist.

Continuing the thread of reasoning internally in Haskell, we represent evaluation by means of a type class with an associated type:

\begin{code}
class EvalLater a where
  type Result a
  leval :: a -> Result a
\end{code}

The most important instance is a recursive instance that strips away an outer \hs{Later} and proceeds to continue to evaluate the result. Additionally, all ground types we wish to consider must be equipped with an appropriate instance, and instances for products, sums, and soforth all arise very mechanically. Importantly, this code is not part of the guarded recursive fragment we are reasoning about, but rather belongs to the equational metatheory, and thus freely pattern matches on \hs{Later}. Some example instances are as follows.

\begin{code}
instance EvalLater a => EvalLater (Later a) where
  type Result (Later a) = Result a
  leval (Later x) = leval x

instance EvalLater Int where
  type Result Int = Int
  leval x = x

instance (EvalLater a, EvalLater b) => EvalLater (a, b) 
  where
    type Result (a, b) = (Result a, Result b)
    leval (x, y) = (leval x, leval y)
\end{code}

As we noted early on, when working in the guarded recursive fragment of the language, recursive structures which do not make use of \hs{Later} are already necessarily finite. As such, evaluation on them remains a purely formal operation. For example, for lists we have:

\begin{code}
instance EvalLater a => EvalLater [a] where
  type Result [a] = [Result a]
  leval xs = fmap leval xs
\end{code}

We will refer to structures which do not make use of \hs{Later} either directly or indirectly as \emph{finite} with regards to the guarded recursive fragment. Finite functors all enjoy the property that \hs{leval = fmap leval}.

The only slight complication arises in the case of guarded datatypes, which may themselves make use of \hs{Later}. In such a case, the need for an associated type becomes very clear, as we are evaluating to an entirely different type, which we have established is isomorphic, rather than to a type obviously syntactically related to the given type. For example, partially infinite streams may be evaluated like so:

\begin{code}
instance EvalLater a => EvalLater (Stream a) where
  type Result (Stream a) = [Result a]
  leval Nil = []
  leval (Cons x  xs) = leval x : leval xs
\end{code}

In general, this construction allows that every recursively guarded variant of a strictly positive datatype can evaluate to the original type.

Corresponding instances can also be written for possibly productive structures, such as below:

\begin{code}
instance EvalLater a => EvalLater (Delay a) where
  type Result (Delay a) = Result a
  leval (Now x) = leval x
  leval (Wait x) = leval x

instance EvalLater a => EvalLater (PStream a) where
   type Result (PStream a) = [Result a]
   leval PNil = []
   leval (PWait x) = leval x
   leval (PCons x xs) = leval x : leval xs
\end{code}

We may now define two terms \hs{x} and {y} of types {a} and {b} as \emph{evaluated-bisimilar}  when there is an equivalence \hs{f :: Result a -> Result b} and further \hs{f (leval x)} = \hs{leval y}. For our purposes, this equivalence of evaluated terms will typically be \hs{id}. 

There is thus a sense in which guarded recursive types may be seen as fibered over equivalence classes of standard Haskell types, providing for each such class a (partially ordered) set of possible refinements of evaluation dependency. For example, the types \hs{Stream a}, \hs{PStream a}, \hs{Later [a]}, and \hs{Later (Later [a])} all "live above" \hs{[a]}. We should note that evaluation is defined on all terms, not just those in the guarded-recursive fragment, and consequently evaluated-bisimilarity as a relation can extend not only to guarded recursive terms, but to general terms.

We call a guarded recursive function \hs{a -> b} a guarded weak bi-equivalence (\emph{gwbeq}) if it sends a term to a bisimilar term. The name is justified because all identities are clearly gwbeq and further, it is straightforward to show that the class of gwbeqs is closed under composition and furthermore satisfies "two out of three" -- that is if  \hs{f :: a -> b} and \hs{g :: b -> c} and two out of three of $\{f, g, g \odot f\}$ are gwbeq, then the third is as well. 
 
 % probably satisfies two out of six, but should we bother
 
Being a gwbeq is substantially weaker than being an isomorphism. In particular, it may not be invertible -- i.e. there is always a map \hs{a -> Later a} and there is no general guarded recursive map in the other direction, either as an actual retract, or even as a gwbeq. However, the possibly productive structures given above are examples of functions where such a wbeq \emph{does} exist. We term types that permit such a map "Later-stable" and equip them with a type class as follows:

\begin{code}
class Stable a where
   wait :: Later a -> a

instance Stable (Delay a) where
   wait = Wait

instance Stable (PStream a) where
   wait = PWait
\end{code}

As an aside, a gwbeq sending a term to a term that is of a stable type may be termed a "stabilization" and consequently, for any type \hs{a}, \hs{Now :: a -> Delay a} is the universal (terminal) stabilization. 

We will also at times consider bi-equivalences which are not necessarily written in the guarded recursive fragment, and will refer to them as evaluated bi-equaivalences (\emph{ebeq}). By definition, \hs{leval} itself is an ebeq.

\section{Properties of predictable functors}
We can now state the desired law of a predicable functor -- the guarded recursive function \hs{predict :: Later (f a) -> f (Later a)} must be a guarded weak bi-equivalence.

Here, we consider some examples, and identify a large class of valid instances. As the instances below show, \hs{Predicable} is closed under composition, and product. Base instances that generate it include exponentiation by a constant, and \hs{Later} itself.
\begin{code}
instance Predictable Later where
  predict = id

instance Predictable ((->) r) where
  predict x = \y -> fmap ($ y) x
   
instance (Predictable f, Predictable g, Functor f) => 
               Predictable (Compose f g) where
  predict = Compose . fmap predict 
               . predict . fmap getCompose

data Prod f g a = Prod {pr1 :: f a, pr2 :: g a}

instance (Predictable f, Predictable g) => 
               Predictable (Prod f g) where
  predict x = Prod 
                  (predict (fmap pr1 x)) 
                  (predict (fmap pr2 x))
\end{code}

Strikingly, \hs{Later} is not closed under sum. In particular, consider the type:

\begin{codex}
data Choice a = C1 a | C2 a
\end{codex}

The outermost constructor of the result of \hs{predict} would necessarily be \hs{C1} or \hs{C2} -- however, by the construction of the \hs{Later} modality, there is no mechanism in the guarded recursive fragment to determine which one. This gives us the inspiration for the name -- a \hs{Predictable} functor is one where, without inspecting the structure, we can "predict" the outermost constructor. In GHC argot, a predictable functor is one where it is safe to preform an irrefutable pattern match.

However, this restriction does not mean that \hs{Predictable} structures are not closed under multiplication by any constant -- in fact, they remain closed precisely under multiplication by \emph{specific} constants -- those which are stable. In particular:

\begin{code}
instance (Stable c) => Predictable ((,) c) where
   predict x = (wait (fst <$> x), snd <$> x)
\end{code}

This turns out to be a bi-implication, and it is easy to show that \hs{((,) c)} is predictable if and only if \hs{c} is stable.

Examples we have examined of datatypes which are not strictly positive all seem to have obstructions to being predictable, and as such we hypothesize that the above instances fully characterize the class of predictable functors.

\section{Bisimilarity by evaluation for arrow types}

% Establish laws for itraversable

The constructions thus far describe properties holding in the guarded recursive fragment. However, an important goal is to  be able to extract reasoning from this fragment and apply it to general terms, and in particular to establish not only that \hs{isequence} can be defined productively, but the conditions under which a \hs{sequence} is productive. To do so we need to extend further our toolkit for reasoning, and in particular, to extend the notion of evaluated bisimilarity to cover types which contain arrows. When we wish to consider evaluation on the function space, the need arises to go in the other direction as well -- that is, to \emph{lift} general values (which are posited to satisfy appropriate productivity conditions) back into the guarded recursive realm. We therefore introduce the following type class to represent inductively the desired inverse: 

\begin{code}
class EvalLater a => LiftLater a where
  llift :: Result a -> a
\end{code}

We require that \hs{llift} be an ebeq -- i.e. that it act as an inverse (up to bisimilarity) of \hs{leval}.

By recursive definition, the \hs{EvalLater} class allows for a meaningful notion of evaluation on arrow types as follows:

\begin{code}  
instance (LiftLater a, EvalLater b) => 
  EvalLater (a -> b) where
     type Result (a -> b) = Result a -> Result b
     leval f = leval . f . llift

instance (LiftLater a, LiftLater b) => 
   LiftLater (a -> b) where
     llift f = llift . f . leval
\end{code}

Inverses (up to bisimilarity) for various defined evaluations follow mechanically. As some examples:

\begin{code}
instance (LiftLater a) => LiftLater (Later a) where
  llift = Later . llift

instance (LiftLater a) => LiftLater (Delay a) where
  llift = Now . llift

instance (LiftLater a, LiftLater b) => 
   LiftLater (a, b) where
     llift (x, y) = (llift x, llift y)

instance LiftLater Int where
  llift = id
\end{code}

It is important to note that these inverses do not, and cannot, send general Haskell terms to guarded recursive terms which are guaranteed productive. In particular, a guarded recursive construction of infinitely nested \hs{Delay} is perfectly reasonable, and under evaluation it goes to $\bot$. Lifting this back in turn does not land in guarded recursive terms (since such terms do not directly contain bottom) but rather in terms which make use of \hs{Later} but exist in general Haskell semantics -- so the lifting in a sense only goes "halfway" back. However, this halfway back intermediate form is all that is necessary for the bookkeeping of the ultimate goal, which is simply that \hs{leval} be defined properly on function types.


% The evaluated type of a predictable applicative functor can productively traverse the evaluated type of an infinite-traversable functor.
% eval . itraverse t . llift -> traverse
% further if traverse exists and is productive, then there exists a predict

% Delay and Guard are an adjoint monad/comonad pair
% make Guard not strong using the Static trick

\section{Examples (and nonexamples) of predictable functors and their traversals}

\begin{comment}
\begin{code}
instance Semigroup w => Semigroup (Delay w)
instance Monoid w => Monoid (Delay w)
instance Semigroup (PStream a)
instance Monoid (PStream a)
\end{code}
\end{comment}

We now revisit the examples from the introduction, showing how the machinery developed can aid in reasoning about infinite traversals. The \hs{Reader} monad is straightforwardly a predictable functor, and so is productive on infinite lists.

\begin{code}
instance Predictable (Reader r) where
  predict x = reader $ \r -> fmap (($ r) . runReader) x
\end{code}

The \hs{Writer w} monad introduces a complication -- to specify a \hs{Predict} instance one must specify a function on the monoid \hs{w} it carries of type \hs{Later w -> w}. As a very general example, when \hs{w} is a monoid, \hs{Delay w} has a natural monoid structure and so we have:

\begin{code}
instance Monoid w => Predictable (Writer (Delay w)) where
  predict x = writer $ (fst . runWriter <$> x, Wait $ snd . runWriter <$> x)
\end{code}

This expresses the fact that, in general, \hs{runWriter . sequence :: [Writer w a] -> ([a], w} will yield a nonbottom second projection only on the occasion that the input list is finite.

However, for specific choices of \hs{w} there are more precise types possible. For instance, there is a straightforward Haskell instance of monoid for \hs{PStream a} -- the type of possibly productive streams. As such, we also have:

\begin{code}
instance Predictable (Writer (PStream a)) where
  predict x = writer $ (fst . runWriter <$> x, PWait $ snd . runWriter <$> x)
\end{code}

This expresses that when the accumulator of a writer is itself a list, then sequencing an infinite list can at times yield a productive second component -- in particular, if there are m writes within the first n terms of the list, then sequencing up to the first n terms of the list will yield m results in the second component.

The \hs{State} monad presents difficulties similar to those of \hs{Writer}, but slightly more complicated. The most naive choice for a \hs{Predict} instance would be \hs{State (Delay s)}. But then at any individual point in a computation, the "real" state would be guarded by a \hs{Delay} operator and so appear not necessarily productively accessible. The property we want to capture is more subtle -- on an infinite list, the second projection of \hs{runState} is indeed bottom, as there is no "final" state. However, at any individual point in a \hs{State} computation, the state \emph{thus far} is immediately accessible. A more granular type to capture the productivity of \hs{State} can be achieved by moving to Uustalu and XX's algebraic decomposition of state into the pair of a reader and writer monads -- aka the \hs{Update} monad.

% motivate predictable functor, state general rule, show how Later is itself not predictable, show that all polynomials generate guarded polynomials, and so generate infinitraversals

\section{Productive applicatives are predictable}

With the above tools in hand, we are ready to discuss the central idea of this paper. We will use lists as our examplar of traversable functors, equipped with the following function:

\begin{code}
sequenceL :: Applicative f => [f a] -> f [a]
sequenceL = fix $ \rec x -> case x of
  [] -> pure []
  (x:xs) -> (:) <$> x <*> rec xs
\end{code}

We say that a finite applicative functor \hs{f} is productive on infinite lists if, when \hs{sequenceL} is instantiated at \hs{f}, the following property holds: $\forall n \in  \mathbb{N}. \, length \circ take \, n \circ sequenceL \neq \bot$.

\begin{theorem}
A finite applicative functor is productive on infinite lists if it may be equipped with a guarded recursive function \hs{predict :: forall a. Later (f a) -> f (Later a)} such that it is a weak bi-equivalence.
\end{theorem}

\begin{proof}

Definite a type class and function as below:

\begin{codex}
class Predict f where
  predict :: Later (f a) -> f (Later a)

sequenceS :: (Applicative f, Predict f) =>
           Stream (f a) -> f (Stream a)
sequenceS = lfix $ \rec x -> case x of
     Nil -> pure Nil
     Cons a s -> 
         Cons 
           <$> a 
           <*> predict (rec <*> s)
\end{codex}

We now must show that \hs{leval sequenceS} is bisimilar to \hs{sequenceL}. To do so, it suffices to show that \hs{leval . sequenceS} is bisimilar to \hs{sequenceL . llift}. We can reason equationally as follows, beginning with making use of the fact that f is a finitary functor. We then substitute the definition of \hs{lfix}. Finally, we push the application of \hs{leval} through the case statement, and make use of the fact that \hs{predict}, \hs{llift} and \hs{leval} are all bisimilarities.

\begin{codex}
leval . sequenceS = fmap leval . sequenceS

= fmap leval . fix $ \rec x -> case x of
     Nil -> pure Nil
     Cons a s -> 
         Cons  
           <$> a 
           <*> predict (pure rec <*> s)
           
= fix $ \rec x -> fmap leval $ case x of
     Nil -> pure Nil
     Cons a s -> 
         Cons  
           <$> a 
           <*> predict (fmap llift . rec <$> s)

= fix $ \rec x -> case x of
     Nil -> pure []
     Cons a s -> 
         (:) 
           <$> a 
           <*> leval (rec <$> s)
\end{codex}

By further equational substitution, it is apparent that this last term is equivalent to \hs{sequenceL . llift}, and so \hs{sequenceS} is bisimilar to \hs{sequenceL}, as desired. Because \hs{sequenceS} is written in the guarded recursive fragment, and so definitionally productive, so too is \hs{sequenceL}.
\end{proof}

It can also be shown that the existence of a function \hs{sequenceS} that evaluates down to \hs{sequenceL} suffices to necessitate a function \hs{predict} which is a weak bi-equivalence.

Given an \hs{x :: Later (f a)}, one can construct a stream \hs{pure _ : x : HNil}. Taking \hs{sequenceS} of this, the resultant type is \hs{f (Stream a)} and mapping the projection of the second element, one gets \hs{f (Later a)}, and by equational reasoning, this is bisimilar to the original \hs{x}.

If we are in a setting such as the typed lambda calculus over which Nakhano specified the later modality, where it is established that all definable productive functions may be written using guarded recursion, then we know that \hs{sequenceL} being productive for a given applicative induces an appropriate \hs{sequenceS} for that applicative, and hence an appropriate \hs{predict}, at which point the implication of the above theorem becomes a bi-implication.

This reverse implication, which is messy when considered over list traversals alone due to partiality, becomes immediate if one moves to a broader class of "infini-traversable" functor. In particular, under a definition where \hs{Later} itself is traversable, then \hs{predict} is simply \hs{lsequence} instantiated at \hs{Later}, i.e. at type \hs{Later (f a) -> f (Later a)}. We will consider such a class in [SECTION TK]



\section{broader class of infinite traversable functors than streams?}

ssequence :: t (f a) -> f (t a). if T itself is Later

\section{The characterization theorem}

A applicative functor is infinite traversable if for all n, `length. take n` is not bottom for sequence [f a]. 

A applicative functor is infinite traversable iff there is a related functor given as a guarded recursive type such that G evaluates to F and G is a predictable applicative functor.

% An applicative functor is infinite traversable iff it can be written in the form (c*a)^r for some fixed types r and c.

All infini-applicatives are generated by product with a constant and reader from a constant.
(can we prove that you can't have a in the covariant position?

suppose we have p : Later f a -> f Later a. and eval = eval . p

we also have eval . fmap eval = eval . p . fmap eval

for functors we can write eval = eval1 . fmap eval

unLater . eval1 . fmap eval = eval1 . fmap eval . p . fmap eval

unLater . eval1 = eval1 . fmap eval . p

p = fmap Later . unLater

-- when can one write that with guarded recursive types?

Now take a to be unit and we have Later f () -> f Later () that is a bisimilarity, i.e. such that eval = eval . p

for all functors we require eval = eval . map eval

so we have eval = eval . map eval

so eval = eval . map (const ())

\section{Related Work}

\section{Future Work and Conclusion}




\end{document}